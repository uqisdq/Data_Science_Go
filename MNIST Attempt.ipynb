{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b6fcf76-40e7-45c9-92c3-28bc87032c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0409bc9-6864-4bcc-bac9-f650ba2ccf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistdataset, mnistinfo = tfds.load(name='mnist',with_info=True,as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5836748-951e-4e7a-bf13-4083bb2b89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnisttrain, mnisttest = mnistdataset['train'],mnistdataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6824a6f1-8929-41d5-85c8-4a6a2da11955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring the number of validation and test\n",
    "num_validation_samples = 0.1*mnistinfo.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples,tf.int64)\n",
    "\n",
    "num_test_samples = mnistinfo.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples,tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a44ec0e-ece4-4db2-8147-9fc8b92c6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Function\n",
    "\n",
    "def scale(image,label) :\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "#make sure the function return image and label (we can use any other scaling functions, ex : sklearn, etc)\n",
    "#dataset.map(*function*) --> Transform the dataset using certain function\n",
    "#.map tupple can also used to rewrite the label of a dataset\n",
    "\n",
    "#scale the datas\n",
    "scaled_train_and_validation = mnisttrain.map(scale)\n",
    "scaled_test_data = mnisttest.map(scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9b85cee-10f7-4208-abf6-fa1f76b73223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data\n",
    "#Because in SGD, the data will be batched, it's better to shuffle the data, so the rows will not be uniform, which is better for propagating/regressing/etc\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "#in some occasions, we cant shuffle the data at once, because of the processing power of the CPU, that is why we shuffled it little by little\n",
    "\n",
    "shuffled_train_and_validation = scaled_train_and_validation.shuffle(BUFFER_SIZE)\n",
    "#.shuffle tuple shuffle an input by a BUFFER_SIZE at a time\n",
    "\n",
    "#take the validation data from shuffled_train_and_validation using .take() tuple\n",
    "validationjadi=shuffled_train_and_validation.take(num_validation_samples)\n",
    "\n",
    "#take the train data from shuffled_train_and_validation using .skip() tuple\n",
    "trainjadi=shuffled_train_and_validation.skip(num_validation_samples)\n",
    "\n",
    "#Do batching\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#Batch using .batch() tuple\n",
    "\n",
    "trainjadi= trainjadi.batch(BATCH_SIZE)\n",
    "validationjadi = validationjadi.batch(num_validation_samples) \n",
    "#-->Validation cuma dibikin satu batch, formalitas aja, karena si modelnya butuh validation di batch juga\n",
    "testjadi=scaled_test_data.batch(num_test_samples)\n",
    "\n",
    "#split the validation into targets and inputs, and transform it into iterator.\n",
    "validationinputs,validationtargets = next(iter(validationjadi))\n",
    "#.iter() transform data into iterator\n",
    "#.next() Load the next data from iterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94e1ebba-9d8e-43bd-8552-523ef659a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlining/Build the NN model\n",
    "\n",
    "#We have 28 x 28 pixel inputs per object, so the input size (width) will be 28 x 28 = 784\n",
    "#We want 2 Hidden layer, with each hidden layer containing 50 I/O\n",
    "#, so there will be 4 layers = inputs, hiddenlayer1, hiddenlayer2, outputs\n",
    "# 4 layers mean Depth = 4\n",
    "#We have 10 Output categories (0,1,...,8,9) --> Output size will be 10\n",
    "    #for the optimal size of the model, further experimentation needed, but according to information above,\n",
    "    #we could say that width = 784 and Depth = 4, is the suboptimal size\n",
    "\n",
    "inputsize = 784\n",
    "outputsize = 10\n",
    "hiddenlayersize = 100\n",
    "\n",
    "#tf.keras.sequential used to determine the layers we want to use\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)), #layer 1\n",
    "                            tf.keras.layers.Dense(hiddenlayersize, activation = 'relu'), #hidden layer 1\n",
    "                            tf.keras.layers.Dense(hiddenlayersize, activation = 'relu'), # hidden layer 2\n",
    "                            tf.keras.layers.Dense(outputsize, activation = 'softmax') # layer 4\n",
    "                            ])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daac36a7-4ad4-4d58-8124-27275b396e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing Optimizer and Loss\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a4860-a13d-4a62-b186-f0ab562fcc72",
   "metadata": {},
   "source": [
    "TRAIN THE MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1761db39-f8c7-468c-b065-607d2e6de696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "540/540 - 12s - loss: 0.3261 - accuracy: 0.9066 - val_loss: 0.1634 - val_accuracy: 0.9528\n",
      "Epoch 2/6\n",
      "540/540 - 5s - loss: 0.1372 - accuracy: 0.9594 - val_loss: 0.1088 - val_accuracy: 0.9673\n",
      "Epoch 3/6\n",
      "540/540 - 5s - loss: 0.0951 - accuracy: 0.9727 - val_loss: 0.0873 - val_accuracy: 0.9747\n",
      "Epoch 4/6\n",
      "540/540 - 5s - loss: 0.0734 - accuracy: 0.9774 - val_loss: 0.0790 - val_accuracy: 0.9762\n",
      "Epoch 5/6\n",
      "540/540 - 5s - loss: 0.0563 - accuracy: 0.9831 - val_loss: 0.0633 - val_accuracy: 0.9812\n",
      "Epoch 6/6\n",
      "540/540 - 5s - loss: 0.0470 - accuracy: 0.9859 - val_loss: 0.0534 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23ebc3f83d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set epochs number\n",
    "NUMEPOCHS = 6\n",
    "\n",
    "#set input, output & other arguments\n",
    "model.fit(trainjadi,epochs=NUMEPOCHS,validation_data=(validationinputs,validationtargets),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34c72b3d-b332-449a-b19f-98adb151dd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.0826 - accuracy: 0.9752\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(testjadi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4081b31a-cb95-4501-a1e6-e39b0a610bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.08. Test accuracy : 97.52%\n"
     ]
    }
   ],
   "source": [
    "print('test loss : {0:.2f}. Test accuracy : {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1720b97-78fd-4d80-a132-38e73979ee43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaabca8-97dc-436a-813d-272be858a01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54dddb-5c07-44ab-957c-65475cd6b584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13c4c4-b688-4c14-a03a-85ab61177789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d8b07-f229-4a45-8bc6-59ded8b8d542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ab3ed-cb93-4520-8881-fb82e2cc6f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486ab2d-cc7a-4d2d-a8b3-aa86df024135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b79ffc-7ae0-4736-b56a-bd644a549674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f14dd-6c70-427d-8d14-44d0b87c94e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
